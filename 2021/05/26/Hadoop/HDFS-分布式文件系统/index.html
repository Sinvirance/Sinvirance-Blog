<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="HDFS 分布式文件系统1. HDFS 简介 The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. Ho">
<meta property="og:type" content="article">
<meta property="og:title" content="HDFS 分布式文件系统">
<meta property="og:url" content="http://example.com/2021/05/26/Hadoop/HDFS-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/index.html">
<meta property="og:site_name" content="Sinvirance Blog">
<meta property="og:description" content="HDFS 分布式文件系统1. HDFS 简介 The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. Ho">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://img.sinvirance.ltd/blog/Hadoop/NameNode 元数据信息.png">
<meta property="og:image" content="http://img.sinvirance.ltd/blog/Hadoop/HDFS 详细架构图.png">
<meta property="og:image" content="http://img.sinvirance.ltd/blog/Hadoop/Windows10Hadoop%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%E6%88%90%E5%8A%9F.png">
<meta property="og:image" content="http://img.sinvirance.ltd//blog/Hadoop/HDFS客户端读数据流程.png">
<meta property="og:image" content="http://img.sinvirance.ltd//blog/Hadoop/HDFS客户端写数据流程.png">
<meta property="og:image" content="http://img.sinvirance.ltd//blog/Hadoop/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86.png">
<meta property="og:image" content="http://img.sinvirance.ltd//blog/Hadoop/%E5%85%83%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6.png">
<meta property="article:published_time" content="2021-05-26T08:50:39.000Z">
<meta property="article:modified_time" content="2021-07-03T07:15:03.304Z">
<meta property="article:author" content="Sinvirance">
<meta property="article:tag" content="分布式">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://img.sinvirance.ltd/blog/Hadoop/NameNode 元数据信息.png">

<link rel="canonical" href="http://example.com/2021/05/26/Hadoop/HDFS-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>HDFS 分布式文件系统 | Sinvirance Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Sinvirance Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Sinvirance Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Lemon</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/sinvirance" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/26/Hadoop/HDFS-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.png">
      <meta itemprop="name" content="Sinvirance">
      <meta itemprop="description" content="有时间绝望的话，还不如吃点好吃的去睡觉呢!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sinvirance Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          HDFS 分布式文件系统
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-26 16:50:39" itemprop="dateCreated datePublished" datetime="2021-05-26T16:50:39+08:00">2021-05-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-03 15:15:03" itemprop="dateModified" datetime="2021-07-03T15:15:03+08:00">2021-07-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS 分布式文件系统"></a>HDFS 分布式文件系统</h2><h3 id="1-HDFS-简介"><a href="#1-HDFS-简介" class="headerlink" title="1. HDFS 简介"></a>1. HDFS 简介</h3><blockquote>
<p>The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is part of the Apache Hadoop Core project.</p>
</blockquote>
<ul>
<li><p>Distributed： 分布式</p>
</li>
<li><p>run on commodity hardware: 运行在通用硬件</p>
</li>
<li><p>highly fault-tolerant: 高容错</p>
</li>
<li><p>highly throughput: 高吞吐</p>
<a id="more"></a>

</li>
</ul>
<h3 id="2-HDFS-的重要概念"><a href="#2-HDFS-的重要概念" class="headerlink" title="2. HDFS 的重要概念"></a>2. HDFS 的重要概念</h3><ul>
<li><p><strong>Master/Slave 架构</strong></p>
<p>HDFS 集群往往是一个 NameNode（HA 架构或者联邦机制可能会有多个 NameNode） + 多个 DataNode 组成</p>
<p>NameNode 是集群的主节点，DataNode 是集群的从节点</p>
</li>
</ul>
<ul>
<li><p><strong>分块存储（block 机制）</strong></p>
<p>HDFS 中的文件在物理上是分块存储在不同节点的，块的大小可通过配置参数来决定，Hadoop2.x 默认的 block 大小为 128M</p>
</li>
<li><p><strong>命名空间（NameSpace）</strong></p>
<p>HDFS 支持传统的层次型文件组织结构（也就是是类似 Windows 或 Linux 的目录形式），用户或者应用程序可以创建目录并将文件保存在目录中</p>
<p>NameNode 负责维护文件系统的名字空间，任何对文件系统的名字空间或者属性的修改都将被 NameNode 记录下来</p>
<p>HDFS 提供给客户一个抽象目录树，访问形式</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs://NameNode的HostName:port/test/input.mp4</span><br><span class="line">hdfs://centos100:9000/test/input.mp4</span><br></pre></td></tr></table></figure></li>
<li><p><strong>NameNode 元数据</strong></p>
<p>目录结构以及文件存储的分块位置信息叫做元数据</p>
<img src="http://img.sinvirance.ltd/blog/Hadoop/NameNode 元数据信息.png" alt="blog/Hadoop/NameNode 元数据信息.png" style="zoom: 67%;" />
</li>
<li><p><strong>DataNode 数据存储</strong></p>
<p>文件的各个 Block 具体存储由 DataNode 节点承担，DataNoda 会定时向 NameNode 汇报自己持有的 Block 信息</p>
</li>
<li><p><strong>副本机制</strong></p>
<p>为了实现容错，文件的所有 Block 都会被进行副本复制，每个文件的 Block 的大小和副本系数都是可配置的，应用程序可以指定某个文件的副本数目，副本系数可以文件创建的时候指定，也可以在之后改变，副本数量默认3个</p>
</li>
<li><p><strong>一次写入，多次读出</strong></p>
<p>HDFS 文件只支持写入一次，除了截断或者追加，并且，在任意时刻，只能由一个写入器，所以，HDFS 不支持并发写入</p>
<p>因此 HDFS 适合用来做大数据的底层存储服务，不适合用来做网盘等应用</p>
</li>
</ul>
<h3 id="3-HDFS-架构"><a href="#3-HDFS-架构" class="headerlink" title="3. HDFS 架构"></a>3. HDFS 架构</h3><img src="http://img.sinvirance.ltd/blog/Hadoop/HDFS 详细架构图.png" alt="HDFS 详细架构图"  />

<ul>
<li>NameNode(nn)：HDFS 集群的 Master 管理者<ul>
<li>维护 HDFS 的名称空间（NameSpace）</li>
<li>维护副本管理策略</li>
<li>记录文件的元数据信息</li>
<li>负责处理客户端的读写请求</li>
</ul>
</li>
<li>DataNode(dn)：NameNode 下达指令，DataNode 实际操作，Slave<ul>
<li>保存实际的数据块 Block</li>
<li>负责数据块的读写</li>
</ul>
</li>
<li>Clinet<ul>
<li>上传文件到 HDFS 时，Clinet 负责将文件切分成 Block 再上传</li>
<li>与 NameNode 交互，获取文件的 元数据信息</li>
<li>读取或者写入文件，与 DataNode 交互</li>
<li>用命令来管理或者访问 HDFS</li>
</ul>
</li>
</ul>
<h3 id="4-HDFS-Client-操作"><a href="#4-HDFS-Client-操作" class="headerlink" title="4. HDFS Client 操作"></a>4. HDFS Client 操作</h3><h4 id="4-1-Shell-命令操作"><a href="#4-1-Shell-命令操作" class="headerlink" title="4.1 Shell 命令操作"></a>4.1 Shell 命令操作</h4><ul>
<li><p>基本语法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs 具体命令 </span><br><span class="line">bin/hadoop fs 具体命令  ## hadoop3.x不推荐使用</span><br></pre></td></tr></table></figure>
</li>
<li><p>常见参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">```shell</span><br><span class="line"> [root@centos100 hadoop-2.9.2]# hdfs dfs</span><br><span class="line"> Usage: hadoop fs [generic options]</span><br><span class="line"> 	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line"> 	[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line"> 	[-checksum &lt;src&gt; ...]</span><br><span class="line"> 	[-chgrp [-R] GROUP PATH...]</span><br><span class="line"> 	[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line"> 	[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line"> 	[-copyFromLocal [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line"> 	[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line"> 	[-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] &lt;path&gt; ...]</span><br><span class="line"> 	[-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line"> 	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line"> 	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line"> 	[-df [-h] [&lt;path&gt; ...]]</span><br><span class="line"> 	[-du [-s] [-h] [-x] &lt;path&gt; ...]</span><br><span class="line"> 	[-expunge]</span><br><span class="line"> 	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line"> 	[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line"> 	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line"> 	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line"> 	[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line"> 	[-help [cmd ...]]</span><br><span class="line"> 	[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]]</span><br><span class="line"> 	[-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line"> 	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line"> 	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line"> 	[-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line"> 	[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line"> 	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line"> 	[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span><br><span class="line"> 	[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line"> 	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line"> 	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line"> 	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line"> 	[-stat [format] &lt;path&gt; ...]</span><br><span class="line"> 	[-tail [-f] &lt;file&gt;]</span><br><span class="line"> 	[-test -[defsz] &lt;path&gt;]</span><br><span class="line"> 	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line"> 	[-touchz &lt;path&gt; ...]</span><br><span class="line"> 	[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line"> 	[-usage [cmd ...]]</span><br><span class="line"> </span><br><span class="line"> Generic options supported are:</span><br><span class="line"> -conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line"> -D &lt;property=value&gt;               define a value for a given property</span><br><span class="line"> -fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.</span><br><span class="line"> -jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line"> -files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line"> -libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span><br><span class="line"> -archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line"> </span><br><span class="line"> The general command line syntax is:</span><br><span class="line"> command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>
</li>
<li><p>HDFS 命令演示</p>
<ul>
<li><p>-help：输出命令参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 hadoop-2.9.2]# vi hdfs.txt </span><br><span class="line">[root@centos100 hadoop-2.9.2]# cat hdfs.txt </span><br><span class="line">hdfs</span><br></pre></td></tr></table></figure></li>
<li><p>-chgrp、-chmod、-chown：修改文件所属权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -ls /</span><br><span class="line">-rw-r--r--   3 root supergroup         12 2021-05-26 22:37 /hdfs.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更改读写权限</span></span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -chmod 666 /hdfs.txt</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -ls /</span><br><span class="line">-rw-rw-rw-   3 root supergroup         12 2021-05-26 22:37 /hdfs.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更改所属用户或者用户组</span></span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -chown root:root /hdfs.txt</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -ls /</span><br><span class="line">-rw-rw-rw-   3 root root               12 2021-05-26 22:37 /hdfs.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更改文件用户组</span></span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -chgrp supergroup /hdfs.txt</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -ls /</span><br><span class="line">-rw-rw-rw-   3 root supergroup         12 2021-05-26 22:37 /hdfs.txt</span><br></pre></td></tr></table></figure></li>
<li><p>-moveFromLocal：从本地移动到 HDFS（剪切）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 hadoop-2.9.2]# vi hdfs.txt </span><br><span class="line">[root@centos100 hadoop-2.9.2]# cat hdfs.txt </span><br><span class="line">hdfs</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -moveFromLocal ./hdfs.txt /</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -ls /</span><br><span class="line">-rw-r--r--   3 root supergroup          5 2021-05-26 22:10 /hdfs.txt</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -cat /hdfs.txt</span><br><span class="line">hdfs</span><br></pre></td></tr></table></figure></li>
<li><p>-appendToFile：追加一个文件到已经存在文件末尾</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 hadoop-2.9.2]# vi hadoop.txt</span><br><span class="line">[root@centos100 hadoop-2.9.2]# cat hadoop.txt</span><br><span class="line">hadoop</span><br><span class="line">=</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -appendToFile hadoop.txt /hdfs.txt</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -cat /hdfs.txt</span><br><span class="line">hdfs</span><br><span class="line">hadoop</span><br></pre></td></tr></table></figure></li>
<li><p>-copyToLocal：从 HDFS 拷贝到 本地</p>
<p>-get：等同于 copyToLocal，从 HDFS 下载到本地</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -copyToLocal /get.txt ./</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -get /get.txt ./</span><br></pre></td></tr></table></figure></li>
<li><p>-copyFromLocal：从本地文件拷贝到 HDFS 中</p>
<p>-put：等同于 copyFromLocal，从 HDFS 上传到本地</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -copyFromLocal put.txt /</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -get put.txt /</span><br></pre></td></tr></table></figure></li>
<li><p>-du：统计文件夹大小信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -du &#x2F;wcinput</span><br><span class="line">61  &#x2F;wcinput&#x2F;wc.txt</span><br></pre></td></tr></table></figure></li>
<li><p>-setrep：设置HDFS 文件副本数量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -ls /hadoop.txt</span><br><span class="line">-rw-r--r--   3 root supergroup          7 2021-05-26 23:03 /hadoop.txt</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -setrep 10 /hadoop.txt</span><br><span class="line">Replication 10 set: /hadoop.txt</span><br><span class="line">[root@centos100 hadoop-2.9.2]# hdfs dfs -ls /hadoop.txt</span><br><span class="line">-rw-r--r--  10 root supergroup          7 2021-05-26 23:03 /hadoop.txt</span><br></pre></td></tr></table></figure>
<p>注意：这里设置的副本数量只是记录在 NameNode 上的元数据中，真正的副本数量取决与实际存在的 DataNode 数量</p>
</li>
</ul>
</li>
</ul>
<h4 id="4-2-Java-API-操作"><a href="#4-2-Java-API-操作" class="headerlink" title="4.2 Java API 操作"></a>4.2 Java API 操作</h4><ol>
<li><p>在 Windows10 中 解压 Hadoop，并配置系统环境变量，注意，Hadoop 和 Java 路径不能出现空格和中文字符</p>
<p>验证 Hadoop 和 JDK 配置</p>
</li>
</ol>
<p>​      <img src="http://img.sinvirance.ltd/blog/Hadoop/Windows10Hadoop%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%E6%88%90%E5%8A%9F.png" alt="Windows10Hadoop环境变量配置成功"></p>
<ol start="2">
<li><p>创建 Maven 项目，引入 Hadoop 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># pom.xml</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>配置 log4j 日志</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># src/resource/log4j.properties</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="comment">### 输出信息到控制台 ###</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span> = <span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.Target</span> = <span class="string">System.out</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span> = <span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span> = <span class="string">[%-5p] %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; method:%l%n%m%n</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span> = <span class="string">[%-5p] %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; method:%l%n%m%n</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建 HdfsClientDemo.java：连接 centos100 Hadoop 集群</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClientDemo</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@BeforeEach</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 1.获取Hadoop集群的Configuration对象 */</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 2.根据Configuration获取HadoopFileSystem对象 */</span></span><br><span class="line">        fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://centos100:9000&quot;</span>), configuration, <span class="string">&quot;root&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@AfterEach</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">/* 4.释放FileSystem对象 */</span></span><br><span class="line">        fs.close();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">/* 3. HDFS api 操作 */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>运行出现：<br>![image-20210530215159223](<a target="_blank" rel="noopener" href="http://img.sinvirance.ltd/blog/Hadoop/winutil.exe">http://img.sinvirance.ltd/blog/Hadoop/winutil.exe</a> no exit.png)</p>
</li>
</ol>
<p>​       将 winutil.exe 放入 %HADOOP_HOME%/bin 目录，并重启电脑</p>
<ol start="6">
<li><p>HDFS 文件系统权限问题</p>
<p>![Hdfs 文件系统权限问题](<a target="_blank" rel="noopener" href="http://img.sinvirance.ltd//blog/Hadoop/Hdfs">http://img.sinvirance.ltd//blog/Hadoop/Hdfs</a> 文件系统权限问题.png)</p>
<ul>
<li><p>指定用户信息获取 FileSystem 对象</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileSystem fs = FileSystem.get(new URI(&quot;hdfs://centos100:9000&quot;), configuration, &quot;root&quot;);</span><br></pre></td></tr></table></figure></li>
<li><p>关闭 HDFS 集群权限校验，并分发到其他节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>放弃 HDFS 的权限校验，生产环境采用 Kerberos 以及其他的安全框架，修改 HDFS 的根目录权限为 777</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 ~]# hdfs dfs -chmod -R 777</span><br></pre></td></tr></table></figure>
<h5 id="HDFS-属性配置"><a href="#HDFS-属性配置" class="headerlink" title="HDFS 属性配置"></a>HDFS 属性配置</h5></li>
</ul>
</li>
</ol>
<ul>
<li><p>使用 Configruation 对象操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 1.获取Hadoop集群的Configuration对象 */</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">/* 配置连接到Hadoop服务器集群 */</span></span><br><span class="line"><span class="comment">// configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://centos100:9000&quot;);</span></span><br><span class="line"><span class="comment">/* 配置副本系数，默认为3 */</span></span><br><span class="line"><span class="comment">// configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;);</span></span><br></pre></td></tr></table></figure></li>
<li><p>使用 API 自带的默认配置：hdfs-default.xml</p>
</li>
<li><p>用户自定义配置文件：hdfs-site.xml</p>
</li>
<li><p>三种方式优先级：Configuration 对象 &gt; 用户自定义配置文件 &gt; API 默认配置文件</p>
</li>
</ul>
<h5 id="API-操作"><a href="#API-操作" class="headerlink" title="API 操作"></a>API 操作</h5><ul>
<li><p>创建文件夹</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdir</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">/* 使用FileSystem对象创建目录 */</span></span><br><span class="line">    fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">&quot;/api_test&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>上传文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyFromLocalToHdfs</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">/* 从本地上传文件到Hdfs */</span></span><br><span class="line">    fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">&quot;D:\\Sinvirance\\WorkSpace\\Hadoop\\client_demo\\test.txt&quot;</span>), </span><br><span class="line">    					 <span class="keyword">new</span> Path(<span class="string">&quot;/api_test/test.txt&quot;</span>)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>下载文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyToHdfsFromLocal</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">/* 从Hdfs下载文件到本地 */</span></span><br><span class="line">    fs.copyToLocalFile(<span class="keyword">true</span>, <span class="keyword">new</span> Path(<span class="string">&quot;/api_test/test.txt&quot;</span>),</span><br><span class="line">                       <span class="keyword">new</span> Path(<span class="string">&quot;D:\\Sinvirance\\WorkSpace\\Hadoop\\client_demo\\test_copy.txt&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>删除文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">/* 递归删除 Hdfs 文件 */</span></span><br><span class="line">    fs.delete(<span class="keyword">new</span> Path(<span class="string">&quot;/api_test&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>查看文件信息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">/* 递归查找文件信息 */</span></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">&quot;/api_test/test.txt&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">        LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/* 获取文件路径 */</span></span><br><span class="line">        String fileName = fileStatus.getPath().getName();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/* 获取文件大小 */</span></span><br><span class="line">        <span class="keyword">long</span> fileStatusLen = fileStatus.getLen();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/* 获取文件权限 */</span></span><br><span class="line">        FsPermission fileStatusPermission = fileStatus.getPermission();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/* 获取文件所属组 */</span></span><br><span class="line">        String fileStatusGroup = fileStatus.getGroup();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/* 获取文件所属用户 */</span></span><br><span class="line">        String fileStatusOwner = fileStatus.getOwner();</span><br><span class="line">        </span><br><span class="line">        System.out.println(fileName + <span class="string">&quot;\t&quot;</span> + fileStatusLen + <span class="string">&quot;\t&quot;</span> + fileStatusPermission + <span class="string">&quot;\t&quot;</span></span><br><span class="line">                + fileStatusOwner + <span class="string">&quot;\t&quot;</span> + fileStatusGroup);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 获取存储的块信息 */</span></span><br><span class="line">        BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (BlockLocation blockLocation: blockLocations) &#123;</span><br><span class="line">            <span class="comment">/* 获取存储块的主机节点信息 */</span></span><br><span class="line">            String[] hosts = blockLocation.getHosts();</span><br><span class="line">            <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;主机名称&quot;</span> + host);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;查找文件信息结束&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>判断文件文件夹</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">FileOrDir</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">/* 判断是文件还是文件夹 */</span></span><br><span class="line">    FileStatus[] fileStatuses = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">        <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;f:&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;d:&quot;</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="IO-流操作"><a href="#IO-流操作" class="headerlink" title="IO 流操作"></a>IO 流操作</h5></li>
<li><p>上传文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * 使用IO流上传文件</span><br><span class="line"> */</span><br><span class="line">@Test</span><br><span class="line">public void uploadFileIO() throws IOException &#123;</span><br><span class="line">    /* 创建本地文件到 输入流 */</span><br><span class="line">    FileInputStream inputStream = </span><br><span class="line">    new FileInputStream(new File(&quot;D:\\Sinvirance\\WorkSpace\\Hadoop\\client_demo\\test.txt&quot;));</span><br><span class="line">    </span><br><span class="line">    /* 创建Hdfs的输出流 */</span><br><span class="line">    FSDataOutputStream outputStream = fs.create(new Path(&quot;/test.txt&quot;));</span><br><span class="line">    </span><br><span class="line">    /* 输入流拷贝到输出流，数组的大小，以及是否默认关闭流，使用默认配置 */</span><br><span class="line">    IOUtils.copyBytes(inputStream, outputStream, configuration);</span><br><span class="line">    </span><br><span class="line">    /* 可以再次手动关闭 */</span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">    IOUtils.closeStream(outputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>下载文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * IO流下载文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">downloadFileIO</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">/* Hdfs文件的输入流 */</span></span><br><span class="line">    FSDataInputStream inputStream = fs.open(<span class="keyword">new</span> Path(<span class="string">&quot;/test.txt&quot;</span>));</span><br><span class="line">    <span class="comment">/* 本地文件的输出流 */</span></span><br><span class="line">    FileOutputStream outputStream = </span><br><span class="line">        <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">&quot;D:\\Sinvirance\\WorkSpace\\Hadoop\\client_demo\\test_copy.txt&quot;</span>));</span><br><span class="line">    <span class="comment">/* 流的拷贝 */</span></span><br><span class="line">    IOUtils.copyBytes(inputStream, outputStream, configuration);</span><br><span class="line">    <span class="comment">/* 可以再次手动关闭 */</span></span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">    IOUtils.closeStream(outputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>seek 定位读取</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用 seek 定位方式读取文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekReadFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">/* Hdfs文件的输入流 */</span></span><br><span class="line">    FSDataInputStream inputStream = fs.open(<span class="keyword">new</span> Path(<span class="string">&quot;/io_seek.txt&quot;</span>));</span><br><span class="line">    <span class="comment">/* 流拷贝数据到控制台 */</span></span><br><span class="line">    IOUtils.copyBytes(inputStream, System.out, <span class="number">4096</span>, <span class="keyword">false</span>);</span><br><span class="line">    <span class="comment">/* 再次读取 */</span></span><br><span class="line">    inputStream.seek(<span class="number">1</span>);</span><br><span class="line">    IOUtils.copyBytes(inputStream, System.out, <span class="number">4096</span>, <span class="keyword">false</span>);</span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="5-HDFS-读写解析"><a href="#5-HDFS-读写解析" class="headerlink" title="5. HDFS 读写解析"></a>5. HDFS 读写解析</h3><h4 id="5-1-HDFS-客户端读数据流程"><a href="#5-1-HDFS-客户端读数据流程" class="headerlink" title="5.1 HDFS 客户端读数据流程"></a>5.1 HDFS 客户端读数据流程</h4><img src="http://img.sinvirance.ltd//blog/Hadoop/HDFS客户端读数据流程.png" style="zoom:;" />

<ol>
<li>客户端通过Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址，<strong>分批返回</strong> HDFS 客户端</li>
<li>挑选一台 DataNode 节点(就近原则，然后随机)，请求读取数据</li>
<li>DataNode 开始传输数据给客户端(从磁盘里面读取数据流，以 <strong>Packet(默认64KB)</strong> 为单位来做校验)</li>
<li>客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件</li>
</ol>
<h4 id="5-2-HDFS客户端写数据流程"><a href="#5-2-HDFS客户端写数据流程" class="headerlink" title="5.2 HDFS客户端写数据流程"></a>5.2 HDFS客户端写数据流程</h4><img src="http://img.sinvirance.ltd//blog/Hadoop/HDFS客户端写数据流程.png" alt="HDFS客户端写数据流程" style="zoom:;" />

<ol>
<li><code>HDFS Client</code> 创建 <code>Distributed FileSystem</code> 对象， <code>Distributed FileSystem</code> 通过RPC远程调用 NameNode 请求上传文件。</li>
<li>NameNode检查 <code>HDFS Client</code> 用户的权限同时检查文件是否存在, 如果有足够的权限以及没有同名的文件，NameNode在文件系统的namespace创建一个该文件(无数据块)，NameNode 返回是否可以上传文件</li>
<li><code>HDFS Client</code> 向 NameNode 请求上传第一个 Block</li>
<li>NameNode 向 <code>HDFS Client</code> 提供所有可以写入Block 副本的DataNode的地址</li>
<li>远程方法调用结束后，<code>Distributed FileSystem</code> 会给 <code>HDFS Client</code> 返回一个输出流 <code>FSDataOutputStream</code>，<code>HDFS Client</code> 使用 <strong>write()</strong> 向HDFS写入数据。客户端通过 <code>FSDataOutputStream</code> 向dn1请求上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，通信管道建立完成。</li>
<li>dn1、dn2、dn3逐级应答 <code>HDFS Client</code></li>
<li> <code>HDFS Client</code> 以 <code>Packet</code> 为单位向写入数据，dn1收到一个 <code>Packet</code> 就会传给dn2，dn2传给dn3； <code>FSDataOutputStream</code> 同时还维护了一个确认队列，用于接收来自 DataNode 的确认信息。客户端每发送一个 <code>Packet</code> ，就将 <code>Packet</code> 信息加入到确认队列。一旦 DataNode 创建了副本，就会发送确认，保证数据的完整性。这些确认信息沿着数据流管道逆流而上，依次经过各个 DataNode 并最终发送到客户端。当客户端收到确认，就将对应的包从确认队列删除，直到收到所有的确认。</li>
<li>当第一个 Block 传输完成后，客户端再次请求 NamaNode 上传第二个 Block（重复 3-7）</li>
</ol>
<p><strong>验证Packet传输</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * HDFS 写数据时验证Packet传输</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUploadPacket</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">/* 准备读取本地文件的输入流 */</span></span><br><span class="line">    FileInputStream fileInputStream =</span><br><span class="line">            <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">&quot;D:\\Sinvirance\\WorkSpace\\Hadoop\\client_demo\\test.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 准备写出数据到 Hdfs 的输出流 */</span></span><br><span class="line">    <span class="comment">//FSDataOutputStream fsDataOutputStream = fs.create(new Path(&quot;/test_packet&quot;), () -&gt; System.out.println(&quot;&amp;&quot;));</span></span><br><span class="line">    FSDataOutputStream fsDataOutputStream = fs.create(<span class="keyword">new</span> Path(<span class="string">&quot;/test_packet&quot;</span>), <span class="keyword">new</span> Progressable() &#123;</span><br><span class="line">        <span class="comment">/* progress 方法每传输64kb数据会执行一次，或者建立传输通道时 */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progress</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;&amp;&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 流拷贝 */</span></span><br><span class="line">    IOUtils.copyBytes(fileInputStream, fsDataOutputStream, configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 关闭输入输出流 */</span></span><br><span class="line">    IOUtils.closeStream(fileInputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataOutputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="6-NN与2NN"><a href="#6-NN与2NN" class="headerlink" title="6. NN与2NN"></a>6. NN与2NN</h3><h4 id="6-1-HFDS-元数据管理机制"><a href="#6-1-HFDS-元数据管理机制" class="headerlink" title="6.1 HFDS 元数据管理机制"></a>6.1 HFDS 元数据管理机制</h4><h5 id="NameNode-如何管理存储元数据"><a href="#NameNode-如何管理存储元数据" class="headerlink" title="NameNode 如何管理存储元数据"></a>NameNode 如何管理存储元数据</h5><p>存储数据的方式来看，要么放到内存中，要么放到磁盘上。HDFS作为一个分布式存储服务，需要处理客户端大量的CRUD请求</p>
<ul>
<li>元数据放入内存中：可以进行高效的数据查询以快速响应客户端的请求，如果namenode发生故障，则元数据会丢失</li>
<li>元数据放入磁盘中：无法实现客户端对元数据信息的任意快速低延迟的响应，但是安全性较高</li>
<li>解决方式：内存 NameNode  + 磁盘 <code>FsImage</code> 镜像文件(快照)</li>
</ul>
<h5 id="元数据怎么实现内存和磁盘的一致性操作"><a href="#元数据怎么实现内存和磁盘的一致性操作" class="headerlink" title="元数据怎么实现内存和磁盘的一致性操作"></a>元数据怎么实现内存和磁盘的一致性操作</h5><ul>
<li>NameNode引入了只能进行追加写入的 <code>edits文件</code>，用于记录HDFS元数据的增删改操作</li>
</ul>
<h5 id="元数据管理流程图"><a href="#元数据管理流程图" class="headerlink" title="元数据管理流程图"></a>元数据管理流程图</h5><p><img src="http://img.sinvirance.ltd//blog/Hadoop/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86.png" alt="元数据管理"></p>
<ul>
<li>第一阶段：<code>NameNode</code> 启动<ul>
<li>第一次启动 <code>NameNode</code>  格式化后，创建 <code>Fsimage</code> 文件和 <code>Edits</code> 文件，不是第一次启动，直接加载编辑日志和镜像文件到内存</li>
<li>客户端对元数据进行增删改的请求</li>
<li>NameNode 记录操作日志，更新滚动 <code>Edits</code>日志</li>
<li>NameNode 在内存中对数据进行增删改</li>
</ul>
</li>
<li>第二阶段：<code>Secondary NameNode</code> 工作<ul>
<li><code>Secondary NameNode</code> 询问 <code>NameNode</code> 是否需要 CheckPoint。直接返回 <code>NameNode</code> 是否执行 CheckPoint 操作结果</li>
<li><code>Secondary NameNode</code> 请求执行 CheckPoint</li>
<li><code>NameNode</code> 滚动正在写的 <code>Edits</code> 日志</li>
<li>将滚动前的编辑日志 <code>Edits_001</code> 和 镜像文件 <code>Fsimage</code> 拷贝到 <code>Secondary NameNode</code> </li>
<li><code>Secondary NameNode</code> 加载编辑日志 <code>Edits_001</code> 和 镜像文件 <code>Fsimage</code> 到内存并合并</li>
<li><code>Secondary NameNode</code> 生成新的镜像文件 <code>fsimage.checkpoint</code></li>
<li>拷贝镜像文件 <code>fsimage.checkpoint</code> 到 <code>NameNode</code></li>
<li>NameNode 将镜像文件 <code>fsimage.checkpoint</code>  重命名为 <code>fsimage</code></li>
</ul>
</li>
</ul>
<h4 id="6-2-Fsimage-与-Edits-文件解析"><a href="#6-2-Fsimage-与-Edits-文件解析" class="headerlink" title="6.2 Fsimage 与 Edits 文件解析"></a>6.2 Fsimage 与 Edits 文件解析</h4><p><code>NameNode</code> 在执行格式化之后，会在 <code>/opt/sin/servers/hadoop-2.9.2/data/tmp/dfs/name/current/</code> 目录下产生如下文件</p>
<p>  <img src="http://img.sinvirance.ltd//blog/Hadoop/%E5%85%83%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6.png" alt="元数据文件"></p>
<ul>
<li><code>Fsimage</code> 文件：是 <code>NameNode</code> 中关于元数据的镜像文件，一般称为检查点，这里包含了 HDFS 文件系统所有目录以及文件相关信息 （Block数量，副本数量，权限等信息）</li>
<li><code>Edits</code> 文件：存储了客户端对 HDFS 文件系统所有的更新操作记录，Client 对 HDFS 文件系统所有的更新操作都会被记录到 <code>Edits</code> 文件中</li>
<li><code>seen_txid</code>：该文件是保存了一个数字，数据对应着最后一个 <code>Edits</code> 文件名的数字，也就是未滚动完成的 <code>Edits</code> 文件</li>
<li><code>VERSION</code>：该文件记录了 <code>NameNode</code> 的一些版本号信息，比如： Custerld， namepaceID 等</li>
</ul>
<h5 id="6-2-1-Fsimage-文件内容"><a href="#6-2-1-Fsimage-文件内容" class="headerlink" title="6.2.1 Fsimage 文件内容"></a>6.2.1 Fsimage 文件内容</h5><p>官方地址：<code>https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html</code></p>
<ol>
<li><p>oiv 和 oev 命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 current]# hdfs</span><br><span class="line">  oiv    apply the offline fsimage viewer to an fsimage</span><br><span class="line">  oev    apply the offline edits viewer to an edits file</span><br></pre></td></tr></table></figure></li>
<li><p>基本语法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oiv -p 文件类型 -i 镜像文件 -o 转化后文件输出路径</span><br></pre></td></tr></table></figure></li>
<li><p>实例操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 current]# hdfs oiv -p XML -i fsimage_0000000000000001105 -o /root/fsimage105.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">fsimage</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layoutVersion</span>&gt;</span>-63<span class="tag">&lt;/<span class="name">layoutVersion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">onDiskVersion</span>&gt;</span>1<span class="tag">&lt;/<span class="name">onDiskVersion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">oivRevision</span>&gt;</span>826afbeae31ca687bc2f8471dc841b66ed2c6704<span class="tag">&lt;/<span class="name">oivRevision</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">NameSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">namespaceId</span>&gt;</span>1740738925<span class="tag">&lt;/<span class="name">namespaceId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">genstampV1</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">genstampV1</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">genstampV2</span>&gt;</span>1117<span class="tag">&lt;/<span class="name">genstampV2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">genstampV1Limit</span>&gt;</span>0<span class="tag">&lt;/<span class="name">genstampV1Limit</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">lastAllocatedBlockId</span>&gt;</span>1073741939<span class="tag">&lt;/<span class="name">lastAllocatedBlockId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">txid</span>&gt;</span>1105<span class="tag">&lt;/<span class="name">txid</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">NameSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">lastInodeId</span>&gt;</span>16682<span class="tag">&lt;/<span class="name">lastInodeId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">numInodes</span>&gt;</span>30<span class="tag">&lt;/<span class="name">numInodes</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span><span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1624205767134<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">permission</span>&gt;</span>root:supergroup:0755<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>9223372036854775807<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>wcinput<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1621961398402<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">permission</span>&gt;</span>root:supergroup:0755<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>16387<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>FILE<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>wc.txt<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replication</span>&gt;</span>3<span class="tag">&lt;/<span class="name">replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1621961398358<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">atime</span>&gt;</span>1622242605106<span class="tag">&lt;/<span class="name">atime</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">preferredBlockSize</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">preferredBlockSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">permission</span>&gt;</span>root:supergroup:0644<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">blocks</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741825<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1001<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>61<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">blocks</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">storagePolicyId</span>&gt;</span>0<span class="tag">&lt;/<span class="name">storagePolicyId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">INodeReferenceSection</span>&gt;</span><span class="tag">&lt;/<span class="name">INodeReferenceSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">SnapshotSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshotCounter</span>&gt;</span>0<span class="tag">&lt;/<span class="name">snapshotCounter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">numSnapshots</span>&gt;</span>0<span class="tag">&lt;/<span class="name">numSnapshots</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">SnapshotSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">INodeDirectorySection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">directory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">parent</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">child</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">directory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">parent</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">child</span>&gt;</span>16387<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">INodeDirectorySection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">FileUnderConstructionSection</span>&gt;</span><span class="tag">&lt;/<span class="name">FileUnderConstructionSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">SecretManagerSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">currentId</span>&gt;</span>0<span class="tag">&lt;/<span class="name">currentId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tokenSequenceNumber</span>&gt;</span>0<span class="tag">&lt;/<span class="name">tokenSequenceNumber</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">numDelegationKeys</span>&gt;</span>0<span class="tag">&lt;/<span class="name">numDelegationKeys</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">numTokens</span>&gt;</span>0<span class="tag">&lt;/<span class="name">numTokens</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">SecretManagerSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">CacheManagerSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">nextDirectiveId</span>&gt;</span>1<span class="tag">&lt;/<span class="name">nextDirectiveId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">numDirectives</span>&gt;</span>0<span class="tag">&lt;/<span class="name">numDirectives</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">numPools</span>&gt;</span>0<span class="tag">&lt;/<span class="name">numPools</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">CacheManagerSection</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">fsimage</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>问题：Fsimage 文件中为什么没有记录块所对应的 <code>DataNode</code> 地址</p>
<p>回答：在集群启动后，<code>NameNode</code> 要求 <code>DataNode</code> 上报数据块信息，并间隔一段时间后再次上报</p>
</li>
</ol>
<h5 id="6-2-2-Edits-文件内容"><a href="#6-2-2-Edits-文件内容" class="headerlink" title="6.2.2 Edits 文件内容"></a>6.2.2 Edits 文件内容</h5><ol>
<li><p>基本语法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oev -p 文件类型 -i 编辑日志 -o 转化后文件输出路径</span><br></pre></td></tr></table></figure></li>
<li><p>实例操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 current]# hdfs oev -p XML -i edits_0000000000000001025-0000000000000001031 -o /root/edit031.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-63<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>1025<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>1026<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16674<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/test.txt<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>3<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1622555548124<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1622555548124<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span>DFSClient_NONMAPREDUCE_-918479977_1<span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span>192.168.72.101<span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>true<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>root<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span>29d61a8b-4283-4200-b167-b9d1c04ff04a<span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>1<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ALLOCATE_BLOCK_ID<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>1027<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741931<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_SET_GENSTAMP_V2<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>1028<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMPV2</span>&gt;</span>1109<span class="tag">&lt;/<span class="name">GENSTAMPV2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD_BLOCK<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>1029<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/test.txt<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741931<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>0<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1109<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span><span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>-2<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_CLOSE<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>1030<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/test.txt<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>3<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1622555548611<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1622555548124<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span><span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span><span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>false<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741931<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>13<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1109<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>root<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_END_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>1031<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">EDITS</span>&gt;</span></span><br></pre></td></tr></table></figure>


</li>
</ol>
<h4 id="6-3-checkpoint-属性设置"><a href="#6-3-checkpoint-属性设置" class="headerlink" title="6.3 checkpoint 属性设置"></a>6.3 checkpoint 属性设置</h4><ul>
<li><p>配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- checkpoint 定时时间为1小时 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 一分钟检查一次操作次数，当操作次数达到100万次，SecondaryNameNode 执行一次 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>1分钟检查一次<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>


</li>
</ul>
<h3 id="7-NN-故障处理"><a href="#7-NN-故障处理" class="headerlink" title="7. NN 故障处理"></a>7. NN 故障处理</h3><p><code>NameNode</code> 故障后，HDFS 集群就无法正常工作，因为 HDFS 文件系统的元数据需要 <code>NameNode</code> 来管理维护并与 Client 交互，如果元数据出现损坏和丢失，同样会导致 <code>NameNode</code> 无法正常工作，进而 HDFS 文件系统无法正常对外提供服务</p>
<ul>
<li>搭建 HDFS 的 HA（高可用）集群，解决 <code>NameNode</code> 的单点故障问题！！（借助 <code>Zookeeper</code> 实现 HA，一个 Active 的 <code>NameNode</code>，一个是 Standby 的 <code>NameNode</code>）</li>
</ul>
<h3 id="8-Hadoop-的限额与归档以及集群安全模式"><a href="#8-Hadoop-的限额与归档以及集群安全模式" class="headerlink" title="8. Hadoop 的限额与归档以及集群安全模式"></a>8. Hadoop 的限额与归档以及集群安全模式</h3><h4 id="8-1-HDFS-文件限额配置"><a href="#8-1-HDFS-文件限额配置" class="headerlink" title="8.1 HDFS 文件限额配置"></a>8.1 HDFS 文件限额配置</h4><p>HDFS 文件的限额配置允许我们以文件大小或者文件个数来限制我们在某个目录下上传的文件数量或者文件内容总量。</p>
<ol>
<li><p>数量限额</p>
   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置文件数量配额为2，只能上传一个文件</span></span><br><span class="line">[root@centos100 ~]# hdfs dfsadmin -setQuota 2 /test</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 清除文件配额</span></span><br><span class="line">[root@centos100 ~]# hdfs dfsadmin -clrQuota /test</span><br></pre></td></tr></table></figure>
</li>
<li><p>空间大小限额</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置空间限制大小为4KB</span></span><br><span class="line">[root@centos100 ~]# hdfs dfsadmin -setSpaceQuota 4k /test</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 上传文件大小超过4k提示</span></span><br><span class="line">[root@centos100 ~]# hdfs dfs -put rsync-script /test</span><br><span class="line">put: The DiskSpace quota of /test is exceeded: quota = 4096 B = 4 KB but diskspace consumed = 402653184 B = 384 MB</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 清除空间限额设置</span> </span><br><span class="line">[root@centos100 ~]# hdfs dfsadmin -clrSpaceQuota /test</span><br></pre></td></tr></table></figure></li>
<li><p>查看配额详情</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 ~]# hdfs dfs -count -q -h /test</span><br><span class="line"> 2               1             4 K             4 K            1            0                  0 /test</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h4 id="8-2-HDFS-安全模式"><a href="#8-2-HDFS-安全模式" class="headerlink" title="8.2 HDFS 安全模式"></a>8.2 HDFS 安全模式</h4><p>HDFS 集群启动时需要加载 <code>Fsimage</code> 以及 <code>edits</code> 文件，而这两个文件都没有记录 block 对应的 <code>DataNode</code> 节点信息，如果此时 Client 请求下载文件等操作，集群是不能工作的，因为此时 HDFS 集群处于安全模式下</p>
<ul>
<li><p><strong>安全模式</strong>是 HDFS 所处的一种特殊状态，在这种状态下，文件系统只接受<strong>读数据请求，不接受删除、修改等变更请求</strong><br><code>NameNode</code> 主节点启动时， HDFS 首先进入安全模式，<code>DataNode</code> 在启动的时候会向 <code>NameNode</code> 汇报可用的 block 等状态，当整个系统达到安全标准时，HDFS 自动离开安全模式。如果 HDFS 出于安全模式下，则文件 block 不能进行任何的副本复制操作，因此达到最小的副本数量要求是基于 <code>DataNode</code> 启动时的状态来判定的，启动时不会再做任何复制（从而达到最小副本数量要求），HDFS 集群刚启动的时候，默认30S钟的时间是出于安全期的，只有过了30s之后，集群脱离了安全期，然后才可以对集群进行操作。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 ~]# hdfs dfsadmin [-safemode enter | leave | get | wait | forceExit]</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h4 id="8-3-Hadoop-归档技术"><a href="#8-3-Hadoop-归档技术" class="headerlink" title="8.3 Hadoop 归档技术"></a>8.3 Hadoop 归档技术</h4><p>由于大量小文件会占用 <code>NameNode</code> 的内存，因此对于 HDFS 来说存储大量小文件造成 <code>Namenode</code> 内存资源的浪费！！</p>
<ul>
<li>Hadoop 存档文件 HAR 文件，是一个更高效的文件存档工具，HAR 文件是由一组文件通过 archive 工具创建而来，在减少了 <code>Namenode</code> 的内存使用的同时，可以对文件进行透明的访问，通俗来说就是 HAR 文件对 <code>Namenode</code> 来说是一个文件减少了内存的浪费，对于实际操作处理文件依然是一个一个独立的文件</li>
</ul>
<p>![HDFS 归档](<a target="_blank" rel="noopener" href="http://img.sinvirance.ltd//blog/Hadoop/HDFS">http://img.sinvirance.ltd//blog/Hadoop/HDFS</a> 归档.png)</p>
<ol>
<li><p>启动 Yarn 集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@centos102 ~]# start-yarn.sh </span><br></pre></td></tr></table></figure></li>
<li><p>归档文件</p>
<p>将 Hadoop 文件系统 /test 目录下的全部文件归档到一个叫 input.har 的归档文件，并放在 / 目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 ~]# hadoop archive -archiveName input.har -p /test  /</span><br></pre></td></tr></table></figure></li>
<li><p>查看归档</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 归档文件内容</span></span><br><span class="line">[root@centos100 ~]# hdfs dfs -ls -R /input.har</span><br><span class="line">-rw-r--r--   3 root supergroup          0 2021-07-02 01:45 /input.har/_SUCCESS</span><br><span class="line">-rw-r--r--   3 root supergroup        258 2021-07-02 01:45 /input.har/_index</span><br><span class="line">-rw-r--r--   3 root supergroup         23 2021-07-02 01:45 /input.har/_masterindex</span><br><span class="line">-rw-r--r--   3 root supergroup         83 2021-07-02 01:45 /input.har/part-0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 归档文件具体文件</span></span><br><span class="line">[root@centos100 ~]# hdfs dfs -ls -R har:///input.har</span><br><span class="line">-rw-r--r--   3 root supergroup         11 2021-07-02 01:41 har:///input.har/a.txt</span><br><span class="line">-rw-r--r--   3 root supergroup         11 2021-07-02 01:41 har:///input.har/test.txt</span><br><span class="line">-rw-r--r--   3 root supergroup         61 2021-07-01 03:51 har:///input.har/wc.txt</span><br></pre></td></tr></table></figure></li>
<li><p>解归档文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@centos100 ~]# hdfs dfs -cp har:///input.har/* /output.har</span><br><span class="line">[root@centos100 ~]# hdfs dfs -ls -R /output.har</span><br><span class="line">-rw-r--r--   3 root supergroup         11 2021-07-02 01:50 /output.har/a.txt</span><br><span class="line">-rw-r--r--   3 root supergroup         11 2021-07-02 01:50 /output.har/test.txt</span><br><span class="line">-rw-r--r--   3 root supergroup         61 2021-07-02 01:50 /output.har/wc.txt</span><br></pre></td></tr></table></figure>

</li>
</ol>

    </div>

    
    
    
        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="tag"># 分布式</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/05/23/Hadoop/Hadoop-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/" rel="prev" title="Hadoop 集群搭建">
      <i class="fa fa-chevron-left"></i> Hadoop 集群搭建
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/05/27/Java/Log4j2-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/" rel="next" title="Log4j2 基本使用">
      Log4j2 基本使用 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="nav-text">HDFS 分布式文件系统</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-HDFS-%E7%AE%80%E4%BB%8B"><span class="nav-text">1. HDFS 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-HDFS-%E7%9A%84%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5"><span class="nav-text">2. HDFS 的重要概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-HDFS-%E6%9E%B6%E6%9E%84"><span class="nav-text">3. HDFS 架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-HDFS-Client-%E6%93%8D%E4%BD%9C"><span class="nav-text">4. HDFS Client 操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Shell-%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C"><span class="nav-text">4.1 Shell 命令操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Java-API-%E6%93%8D%E4%BD%9C"><span class="nav-text">4.2 Java API 操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#HDFS-%E5%B1%9E%E6%80%A7%E9%85%8D%E7%BD%AE"><span class="nav-text">HDFS 属性配置</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#API-%E6%93%8D%E4%BD%9C"><span class="nav-text">API 操作</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#IO-%E6%B5%81%E6%93%8D%E4%BD%9C"><span class="nav-text">IO 流操作</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-HDFS-%E8%AF%BB%E5%86%99%E8%A7%A3%E6%9E%90"><span class="nav-text">5. HDFS 读写解析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-HDFS-%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="nav-text">5.1 HDFS 客户端读数据流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="nav-text">5.2 HDFS客户端写数据流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-NN%E4%B8%8E2NN"><span class="nav-text">6. NN与2NN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-HFDS-%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6"><span class="nav-text">6.1 HFDS 元数据管理机制</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#NameNode-%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E5%AD%98%E5%82%A8%E5%85%83%E6%95%B0%E6%8D%AE"><span class="nav-text">NameNode 如何管理存储元数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E5%86%85%E5%AD%98%E5%92%8C%E7%A3%81%E7%9B%98%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E6%93%8D%E4%BD%9C"><span class="nav-text">元数据怎么实现内存和磁盘的一致性操作</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="nav-text">元数据管理流程图</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-Fsimage-%E4%B8%8E-Edits-%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90"><span class="nav-text">6.2 Fsimage 与 Edits 文件解析</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#6-2-1-Fsimage-%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9"><span class="nav-text">6.2.1 Fsimage 文件内容</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-2-2-Edits-%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9"><span class="nav-text">6.2.2 Edits 文件内容</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-checkpoint-%E5%B1%9E%E6%80%A7%E8%AE%BE%E7%BD%AE"><span class="nav-text">6.3 checkpoint 属性设置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-NN-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86"><span class="nav-text">7. NN 故障处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-Hadoop-%E7%9A%84%E9%99%90%E9%A2%9D%E4%B8%8E%E5%BD%92%E6%A1%A3%E4%BB%A5%E5%8F%8A%E9%9B%86%E7%BE%A4%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F"><span class="nav-text">8. Hadoop 的限额与归档以及集群安全模式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-HDFS-%E6%96%87%E4%BB%B6%E9%99%90%E9%A2%9D%E9%85%8D%E7%BD%AE"><span class="nav-text">8.1 HDFS 文件限额配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-HDFS-%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F"><span class="nav-text">8.2 HDFS 安全模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-Hadoop-%E5%BD%92%E6%A1%A3%E6%8A%80%E6%9C%AF"><span class="nav-text">8.3 Hadoop 归档技术</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sinvirance"
      src="/images/header.png">
  <p class="site-author-name" itemprop="name">Sinvirance</p>
  <div class="site-description" itemprop="description">有时间绝望的话，还不如吃点好吃的去睡觉呢!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Sinvirance" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Sinvirance" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sinvirance</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
